\section{Introduction}
Neural controlled differential equations (Neural CDEs) \citep{kidger2020neural} are the continuous-time analogue to a recurrent neural network (RNN), and provide a natural method for modelling temporal dynamics with neural networks.

Neural CDEs are similar to neural ordinary differential equations (Neural ODEs), as popularised by \citet{neural2018ode}. A Neural ODE is determined by its initial condition, without a direct way to modify the trajectory given subsequent observations. In contrast the vector field of a Neural CDE depends upon the time-varying data, so that changes in the data provoke in the local dynamics of the system.

\subsection{Controlled Differential Equations}
We begin by recalling the definition of a CDE.

Let $a, b \in \mathbb{R}$ with $a < b$, and let $v, w \in \mathbb{N}$. Let $\xi \in \mathbb{R}^w$. Let $X \colon [a, b] \to \mathbb{R}^v$ be a continuous function of bounded variation (which is for example implied by it being Lipschitz), and let $f~\colon~\mathbb{R}^w~\to~\mathbb{R}^{w \times v}$ be continuous.

Then we may define $Z: [a, b] \to \mathbb{R}^w$ as the unique solution of the \emph{controlled differential equation}
\begin{equation}
    Z_a = \xi,\qquad Z_t = Z_a + \int^{t}_{a} f(Z_s) \dby X_s \quad \text{for } t \in (a, b],
    \label{eq:kidger_cde}
\end{equation}
where the integral is a Riemann-Stieltjes integral. The notation ``$f(Z_s) \dby X_s$'' denotes a matrix-vector product, and if $X$ is differentiable then $\int_a^t f(Z_s) \dby X_s = \int_a^t f(Z_s) \frac{\dby X}{\dby s}(s) \dby s$.
% Add something about universality

If in equation (\ref{eq:kidger_cde}), $\dby X_s$ was replaced with $\dby s$, then the equation would just be an ODE. Using $\dby X_s$ causes the solution to depend continuously on changes in $X$.

\subsection{Neural Controlled Differential Equations}
We recall the definition of a Neural CDE as in \citet{kidger2020neural}.

Define a time series $\mathbf{x}$ as a collection of points $x_i \in \mathbb{R}^{v-1}$ with corresponding time-stamps $t_i \in \mathbb{R}$ such that $\mathbf{x} = ((t_0, x_0), (t_1, x_1), ..., (t_n, x_n))$, and $t_0 < ... < t_n$.

Let $X \colon [t_0, t_n] \to \reals^{v}$ be some interpolation of the data such that $X_{t_i} = (t_i, x_i)$. \citet{kidger2020neural} use natural cubic splines. Here we will actually end up finding piecewise linear interpolation to be a more convenient choice. (We avoid issues with adaptive solvers as discussed in \citet[Appendix A]{kidger2020neural} simply by using fixed solvers.) % TODO: discuss the linear interpolation paper once it's out.

Let $\xi_\theta \colon \reals^{v} \to \reals^w$ and $f_\theta \colon \reals^w \to \reals^{w \times v}$ be neural networks. Let $\ell_\theta \colon \reals^w \to \reals^{q}$ be linear, for some output dimension $q \in \naturals$. Here $\theta$ is used to denote dependence on learnable parameters.

We define $Z$ as the hidden state and $Y$ as the output of a \textit{neural controlled differential equation} driven by $X$ if
\begin{equation}
    Z_{t_0} = \xi_\theta(t_0, x_0), \quad \text{with} \quad  Z_t = Z_{t_0} + \int^{t}_{t_0} f_\theta(Z_s) \dby X_s \quad \text{and} \quad Y_t = \ell_\theta(Z_t) \quad \text{for } t \in (t_0, t_n].
    \label{eq:kidger_ncde}
\end{equation}

That is -- just like an RNN - we have evolving hidden state $Z$, which we take a linear map from to produce an output. This formulation is a universal approximator \citep[Appendix B]{kidger2020neural}. The output may be either the time-evolving $Y_t$ or just the final $Y_{t_n}$. This is then fed into a loss function ($L^2$, cross entropy, \ldots) and trained via stochastic gradient descent in the usual way.

The question remains how to compute the integral of equation (\ref{eq:kidger_ncde}). \citet{kidger2020neural} let
\begin{equation}
    g_{\theta, X}(Z, s) = f_\theta(Z) \frac{\dby X}{\dby s}(s),
    \label{eq:kidger_g_X}
\end{equation}
where the right hand side denotes a matrix multiplication, and then note that the integral can be written as
\begin{equation}
     Z_t = Z_{t_0} + \int^{t}_{t_0} g_{\theta, X}(Z_s, s) \dby s.
     \label{eq:kidger_cde_evaluation}
\end{equation}
This reduces the CDE to an ODE, so that existing tools for Neural ODEs may be used to evaluate this, and to backpropagate.

% To alleviate this, we can simply update the hidden state less frequently by skipping some of the time steps in the series. That is, we instead consider times $r_1,...,r_m$ where $r_m = t_n$, $r_i \geq t_i \; \forall i$ with typically $m << n$, and update with
% \begin{equation}
%      Z_{r_{i+1}} = Z_{r_i} + \int^{r_{i+1}}_{r_i} g_{\theta, X}(Z, s) \dby s.
%      \label{eq:kidger_cde_multi_data_update}
% \end{equation}
% Whilst this solves the training time time issue, it also results in a loss of information over each step as it utilises only information about the pointwise derivative at a fixed number of points.

\subsection{Applications}
By moving from the discrete-time formulation of an RNN to the continuous-time formulation of a Neural CDE, then all kinds of time series data are put on the same footing, whether it is regular or irregularly sampled, and whether or not it has missing values. The interpolation procedure for $X$ works regardless. Informative missingness is not sacrificed and can be incorporated by appending extra channels as usual.

Besides this, the continuous-time or differential equation formulation may be useful in applications where such models are explicitly desired, as when modelling physics.

% TODO: Add the NSDE paper once it's out.

\subsection{Contributions}
Neural CDEs, as with RNNs, begin to break down for long time series. Training loss/accuracy worsens, and training time becomes prohibitive due to the sheer number of forward operations within each training epoch.

Here, we propose to use the ``log-ODE method'', which is a numerical method for the solution of CDEs. In the CDE literature this is used as a way to handle roughness in $X$. Here, however, it instead becomes a principled way to trade length for channels. Loosely speaking, long time series are reduced to higher-dimensional short time series, and the problems attendant with length are alleviated. Training times may be improved from roughly days to roughly minutes, model performance is improved, and memory requirements are reduced.

The resulting scheme has two very neat interpretations. In terms of numerical differential equation solvers, this corresponds to taking integration steps larger than the discretisation the data, whilst incorporating sub-step information through additional terms.\footnote{For the reader familiar with numerical methods for SDEs, this is akin to the additional correction term in Milstein's method as compared to Euler--Maruyama.} In terms of machine learning, this corresponds to binning the data prior to running a Neural CDE, with bin statistics carefully chosen to extract precisely the information most relevant to solving a CDE. The method may be implemented as a relatively straightforward data preprocessing step.

% When evaluating equation (\ref{eq:kidger_cde_multi_data_update}) using the definition of $g_{\theta, X}$ as in equation (\ref{eq:kidger_g_X}), information about the structure of the data is only observed through the point-wise derivative of the interpolated data at a fixed number of points. The log-ODE method replaces the derivative function with something called the \textit{log-signature} of the path which provides essentially a compression of the data that holds more information than simply the derivative. We will show that in equation (\ref{eq:kidger_cde_multi_data_update}) we will instead  use
% \begin{equation}
%     g_{\theta, X}(Z, s) = f_\theta(Z) \logsig^N_{r_i, r_{i+1}}(X_s),
%     \label{eq:log-ode_update_simple}
% \end{equation}
% where this $\logsig^N_{r_i, r_{i+1}}(X_s)$ term is what provides us with additional information about the structure of the data over the interval resulting in a more accurate solution for $Z_{r_{i+1}}$. Furthermore, we find that when updating over a single step, the log-signature term becomes equivalent to the derivative as in equation (\ref{eq:kidger_g_X}).


% The proposed method of updating the hidden step using a summary of multiple pieces of data in one step will provide us with the following benefits from the original Neural CDE approach:
% \begin{enumerate}
%     \item \textbf{Faster training times} Solving using integration steps larger than the discretisation of the data, but retaining information about the structure of the path over these steps via the log-signature, this often results in significantly reduced training times whilst retaining accuracy. 
%     \item \textbf{Improved learning from long sequences} A common drawback with sequential models such as the RNN is their failure to learn from long sequences \citep{salehinejad2017recent, cho2014properties}. Solving over larger steps results in less overall steps in the sequential model, alleviating this problem. We will show that this often leads to longer steps resulting in higher accuracies, which we believe is in part due to this issue of long-term dependencies. 
%     \item \textbf{Memory efficiency} High frequency data can contain significant redundant information. Conversion of this data onto local descriptions over intervals can result in significantly reduced memory usage for data storage without hampering model performance.
%     \item \textbf{Robustness to  high frequency inputs and noise} The theory of solutions to CDEs is well-understood and detailed by Rough Path Theory (introduced in \citet{roughpath1998def}) where a centrepiece result, the ``Universal Limit Theorem'', guarantees the continuity of the solution map $(X_t, Y_0)\mapsto Y_t$. For applications, this means our modelling framework can process challenging data sets in a stable manner (such as those with high frequency or noisy signals). For further details, we direct the reader to \citet{roughpath2007notes}.
% \end{enumerate}
% The model additionally retains all of the benefits of Neural CDEs such as the ability to train via the adjoint method. 