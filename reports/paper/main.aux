\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{kidger2020neural}
\citation{neural2018ode}
\citation{kidger2020neural}
\citation{kidger2020neural}
\citation{kidger2020neural}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Controlled Differential Equations}{1}{subsection.1.1}}
\newlabel{eq:kidger_cde}{{1}{1}{Controlled Differential Equations}{equation.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Neural Controlled Differential Equations}{1}{subsection.1.2}}
\citation{kidger2020neural}
\citation{kidger2020neural}
\newlabel{eq:kidger_ncde}{{2}{2}{Neural Controlled Differential Equations}{equation.1.2}{}}
\newlabel{eq:kidger_g_X}{{3}{2}{Neural Controlled Differential Equations}{equation.1.3}{}}
\newlabel{eq:kidger_cde_evaluation}{{4}{2}{Neural Controlled Differential Equations}{equation.1.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Applications}{2}{subsection.1.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Contributions}{2}{subsection.1.4}}
\citation{hambly2010sigunique}
\citation{roughpath2007notes}
\citation{bonnier2019sig}
\citation{reizenstein2017logsig,reizenstein2018iisignature}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \textbf  {Left:} The original Neural CDE formulation. The path $X_t$ is quickly varying, meaning a lot of integration steps are needed to resolve it. \textbf  {Right:} The log-ODE method. The logsignature path is more slowly varying (in a higher dimensional space), and needs fewer integration steps to resolve.\relax }}{3}{figure.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:ncde_plots}{{1}{3}{\textbf {Left:} The original Neural CDE formulation. The path $X_t$ is quickly varying, meaning a lot of integration steps are needed to resolve it. \textbf {Right:} The log-ODE method. The logsignature path is more slowly varying (in a higher dimensional space), and needs fewer integration steps to resolve.\relax }{figure.caption.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Theory}{3}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Signatures and Logsignatures}{3}{subsection.2.1}}
\@writefile{toc}{\contentsline {paragraph}{Signature transform}{3}{section*.2}}
\newlabel{eq:path_signature}{{5}{3}{Signature transform}{equation.2.5}{}}
\newlabel{eq:sig_moments}{{6}{3}{Signature transform}{equation.2.6}{}}
\newlabel{eq:truncated_path_signature}{{7}{3}{Signature transform}{equation.2.7}{}}
\@writefile{toc}{\contentsline {paragraph}{Logsignature transform}{3}{section*.3}}
\citation{janssen2012thesis,lyons2014streams}
\citation{logode2014estimate}
\citation{gyurko2008logode,flint2015logode,foster2020poly}
\citation{magnus2008expansion}
\citation{pinkus,deepandnarrow}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}The Log-ODE Method}{4}{subsection.2.2}}
\newlabel{eq:log-ode}{{8}{4}{The Log-ODE Method}{equation.2.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Neural CDEs via the log-ODE method}{4}{subsection.2.3}}
\@writefile{toc}{\contentsline {paragraph}{Lie bracket structure}{4}{section*.4}}
\@writefile{toc}{\contentsline {paragraph}{Hyperparameters}{4}{section*.5}}
\@writefile{toc}{\contentsline {paragraph}{Overall}{4}{section*.6}}
\citation{signatory}
\citation{pytorch}
\@writefile{toc}{\contentsline {section}{\numberline {3}Computation}{5}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Method}{5}{subsection.3.1}}
\@writefile{toc}{\contentsline {paragraph}{Data}{5}{section*.7}}
\@writefile{toc}{\contentsline {paragraph}{Logsignature transform}{5}{section*.8}}
\@writefile{toc}{\contentsline {paragraph}{Neural CDEs}{5}{section*.9}}
\newlabel{eq:logsig_time_series}{{10}{5}{Neural CDEs}{equation.3.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Discussion}{5}{subsection.3.2}}
\@writefile{toc}{\contentsline {paragraph}{Binning}{5}{section*.10}}
\citation{kidger2020neural}
\citation{bagnall16bakeoff}
\@writefile{toc}{\contentsline {paragraph}{Variation speed}{6}{section*.11}}
\@writefile{toc}{\contentsline {paragraph}{Length/channel trade-off}{6}{section*.12}}
\@writefile{toc}{\contentsline {paragraph}{Ease of implementation}{6}{section*.13}}
\@writefile{toc}{\contentsline {paragraph}{Memory efficiency}{6}{section*.14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}The depth and step hyperparameters}{6}{subsection.3.3}}
\newlabel{subsec:tradeoff}{{3.3}{6}{The depth and step hyperparameters}{subsection.3.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experiments}{6}{section.4}}
\newlabel{sec:experiments}{{4}{6}{Experiments}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Classification with EigenWorms}{6}{subsection.4.1}}
\citation{MonashTSRegressionArchive}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \textbf  {Left:} Heatmap of accuracies on the EigenWorms dataset for differing step sizes and depths. \textbf  {Right:} Log-log plot of the elapsed time of the algorithm against the step size.\relax }}{7}{figure.caption.15}}
\newlabel{fig:eigenworms}{{2}{7}{\textbf {Left:} Heatmap of accuracies on the EigenWorms dataset for differing step sizes and depths. \textbf {Right:} Log-log plot of the elapsed time of the algorithm against the step size.\relax }{figure.caption.15}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Performance on the UEA EigenWorms dataset for depths 1-3 and a selection of step sizes. Bold denotes that the model was the top performer for that step-size.\relax }}{7}{table.caption.16}}
\newlabel{tab:eigenworms}{{1}{7}{Performance on the UEA EigenWorms dataset for depths 1-3 and a selection of step sizes. Bold denotes that the model was the top performer for that step-size.\relax }{table.caption.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Estimating Vitals Signs from PPG and ECG Data}{7}{subsection.4.2}}
\citation{campos2017skip}
\citation{graves2012supervised,de2015survey}
\citation{liao2019learning}
\citation{kidger2020neural}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces The RMSE scores on the test set for each of the vitals signs prediction tasks (RR, HR, SpO$_2$) on the BIDMC dataset. The memory usage is given as the mean over all three of the tasks as it was approximately the same for any task for a given depth and step. The bold values denote the algorithm with the lowest test set loss for a fixed step size for each task.\relax }}{8}{table.caption.17}}
\newlabel{tab:bidmc}{{2}{8}{The RMSE scores on the test set for each of the vitals signs prediction tasks (RR, HR, SpO$_2$) on the BIDMC dataset. The memory usage is given as the mean over all three of the tasks as it was approximately the same for any task for a given depth and step. The bold values denote the algorithm with the lowest test set loss for a fixed step size for each task.\relax }{table.caption.17}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Limitations}{8}{section.5}}
\@writefile{toc}{\contentsline {paragraph}{Number of hyperparameters}{8}{section*.18}}
\@writefile{toc}{\contentsline {paragraph}{Number of input channels}{8}{section*.19}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Related Work}{8}{section.6}}
\bibdata{bibliography}
\bibstyle{style/iclr2020_conference}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusions}{9}{section.7}}
\citation{friz2010multidimensional}
\@writefile{toc}{\contentsline {section}{\numberline {A}An introduction to the log-ODE method for controlled differential equations}{10}{appendix.A}}
\newlabel{apx:logode}{{A}{10}{An introduction to the log-ODE method for controlled differential equations}{appendix.A}{}}
\newlabel{eq:cde_for_logode}{{11}{10}{An introduction to the log-ODE method for controlled differential equations}{equation.A.11}{}}
\newlabel{def:tensoralgebras}{{A.1}{10}{An introduction to the log-ODE method for controlled differential equations}{theorem.A.1}{}}
\newlabel{eq:tensoradd}{{12}{10}{An introduction to the log-ODE method for controlled differential equations}{equation.A.12}{}}
\newlabel{eq:tensormult}{{13}{10}{An introduction to the log-ODE method for controlled differential equations}{equation.A.13}{}}
\newlabel{def:signature_appendix}{{A.2}{10}{An introduction to the log-ODE method for controlled differential equations}{theorem.A.2}{}}
\newlabel{eq:fullsig}{{14}{10}{An introduction to the log-ODE method for controlled differential equations}{equation.A.14}{}}
\newlabel{eq:truncsig}{{15}{10}{An introduction to the log-ODE method for controlled differential equations}{equation.A.15}{}}
\newlabel{def:tensor_log}{{A.3}{11}{The logarithm of a formal series}{theorem.A.3}{}}
\newlabel{eq:fulltensorlogseries}{{16}{11}{The logarithm of a formal series}{equation.A.16}{}}
\newlabel{eq:trunctensorlogseries}{{17}{11}{The logarithm of a truncated series}{equation.A.17}{}}
\newlabel{def:logsig_appendix}{{A.5}{11}{An introduction to the log-ODE method for controlled differential equations}{theorem.A.5}{}}
\newlabel{def:vect_derivative}{{A.6}{11}{Vector field derivatives}{theorem.A.6}{}}
\newlabel{eq:taylor_def}{{18}{11}{\textbf {The Taylor method}}{equation.A.18}{}}
\newlabel{def:logode}{{A.8}{11}{\textbf {The Log-ODE method}}{theorem.A.8}{}}
\newlabel{eq:first_logode_def}{{19}{11}{\textbf {The Log-ODE method}}{equation.A.19}{}}
\citation{roughpath2007notes}
\citation{lyons2014streams}
\citation{logode2014estimate}
\citation{wongzakai1965}
\newlabel{diag:logode}{{\caption@xref {diag:logode}{ on input line 116}}{12}{An introduction to the log-ODE method for controlled differential equations}{figure.caption.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Illustration of the log-ODE and Taylor methods for controlled differential equations.\relax }}{12}{figure.caption.22}}
\newlabel{eq:second_logode_def}{{20}{12}{\textbf {The Log-ODE method}}{equation.A.20}{}}
\newlabel{eq:IGBM}{{21}{12}{An application for SDE simulation}{equation.A.21}{}}
\citation{foster2020poly}
\citation{logode2014estimate}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}The generalized log-ODE method}{13}{subsection.A.1}}
\newlabel{eq:general_logode_def}{{22}{13}{The generalized log-ODE}{equation.A.22}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Convergence of the log-ODE method for rough differential equations}{13}{appendix.B}}
\newlabel{apx:logodeconv}{{B}{13}{Convergence of the log-ODE method for rough differential equations}{appendix.B}{}}
\newlabel{thm:logODEthm}{{B.1}{13}{Lemma 15 in \cite {logode2014estimate}}{theorem.B.1}{}}
\newlabel{eq:RDE}{{23}{13}{Lemma 15 in \cite {logode2014estimate}}{equation.B.23}{}}
\citation{tensorproducts2002book}
\citation{roughpath2007notes}
\newlabel{eq:roughpathincrements}{{24}{14}{Lemma 15 in \cite {logode2014estimate}}{equation.B.24}{}}
\newlabel{eq:rpconvege}{{25}{14}{Lemma 15 in \cite {logode2014estimate}}{equation.B.25}{}}
\newlabel{eq:rpmetric}{{26}{14}{Lemma 15 in \cite {logode2014estimate}}{equation.B.26}{}}
\newlabel{eq:lipgamma}{{27}{14}{Lemma 15 in \cite {logode2014estimate}}{equation.B.27}{}}
\newlabel{eq:standardlogode}{{28}{14}{Lemma 15 in \cite {logode2014estimate}}{equation.B.28}{}}
\newlabel{eq:logodevectfield}{{29}{14}{Lemma 15 in \cite {logode2014estimate}}{equation.B.29}{}}
\citation{friz2010multidimensional}
\citation{gyurko2008thesis}
\newlabel{eq:local_logodeestimate}{{30}{15}{Lemma 15 in \cite {logode2014estimate}}{equation.B.30}{}}
\newlabel{eq:rpnorm}{{31}{15}{Lemma 15 in \cite {logode2014estimate}}{equation.B.31}{}}
\newlabel{thm:linearexistance}{{B.3}{15}{Theorem 10.57 in \cite {friz2010multidimensional}}{theorem.B.3}{}}
\newlabel{eq:linearRDEbound}{{32}{15}{Theorem 10.57 in \cite {friz2010multidimensional}}{equation.B.32}{}}
\newlabel{eq:linearODEbound}{{33}{15}{Convergence of the log-ODE method for rough differential equations}{equation.B.33}{}}
\newlabel{rmk:linear_rmk}{{B.6}{15}{Convergence of the log-ODE method for rough differential equations}{theorem.B.6}{}}
\newlabel{eq:standardlogode2}{{34}{16}{Convergence of the log-ODE method for rough differential equations}{equation.B.34}{}}
\newlabel{eq:global_logodeestimate}{{35}{16}{Convergence of the log-ODE method for rough differential equations}{equation.B.35}{}}
\@writefile{toc}{\contentsline {section}{\numberline {C}Experimental details}{16}{appendix.C}}
\newlabel{apx:experiments}{{C}{16}{Experimental details}{appendix.C}{}}
\@writefile{toc}{\contentsline {paragraph}{Code}{16}{section*.23}}
\@writefile{toc}{\contentsline {paragraph}{Data splits}{16}{section*.24}}
\@writefile{toc}{\contentsline {paragraph}{Normalisation}{16}{section*.25}}
\@writefile{toc}{\contentsline {paragraph}{Architecture}{16}{section*.26}}
\@writefile{toc}{\contentsline {paragraph}{Activation functions}{16}{section*.28}}
\@writefile{toc}{\contentsline {paragraph}{ODE Solver}{16}{section*.29}}
\@writefile{toc}{\contentsline {paragraph}{Computing infrastructure}{16}{section*.30}}
\@writefile{toc}{\contentsline {paragraph}{Optimiser}{16}{section*.31}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Overview of the hidden state update network structure. We give the dimensions at each layer in the top right hand corner of each box.\relax }}{17}{figure.caption.27}}
\newlabel{fig:network_diagram}{{4}{17}{Overview of the hidden state update network structure. We give the dimensions at each layer in the top right hand corner of each box.\relax }{figure.caption.27}{}}
\@writefile{toc}{\contentsline {paragraph}{Hyperparameter selection}{17}{section*.32}}
\@writefile{toc}{\contentsline {section}{\numberline {D}Experimental Results}{17}{appendix.D}}
\newlabel{apx:results}{{D}{17}{Experimental Results}{appendix.D}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Heatmaps of the loss on the test set for the all BIDMC problems and the and a log-log plot of the elapsed time of the algorithm against the step size. Recall that lower is better here unlike for EigenWorms.\relax }}{18}{figure.caption.37}}
\newlabel{fig:bidmc_heatmap}{{5}{18}{Heatmaps of the loss on the test set for the all BIDMC problems and the and a log-log plot of the elapsed time of the algorithm against the step size. Recall that lower is better here unlike for EigenWorms.\relax }{figure.caption.37}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Hyperparamter selection results for the EigenWorms dataset. The blue values denote the selected hyperparameters.\relax }}{19}{table.caption.33}}
\newlabel{tab:eigenworms_hyper}{{3}{19}{Hyperparamter selection results for the EigenWorms dataset. The blue values denote the selected hyperparameters.\relax }{table.caption.33}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Hyperparameter selection results for each problem of the BIDMC dataset. The bold values denote the selected hyperparameters for each vitals sign problem. Note that RR and SpO2 had the same parameters selected, hence why only two lines are given in bold.\relax }}{20}{table.caption.34}}
\newlabel{tab:bidmc_hyper}{{4}{20}{Hyperparameter selection results for each problem of the BIDMC dataset. The bold values denote the selected hyperparameters for each vitals sign problem. Note that RR and SpO2 had the same parameters selected, hence why only two lines are given in bold.\relax }{table.caption.34}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Test set accuracy (in \%), memory usage and training time on the UEA EigenWorms dataset for depths 1-3 and a small selection of step sizes. The bold values denote that the model was the top performer for that step size.\relax }}{21}{table.caption.35}}
\newlabel{tab:eigenworms_all}{{5}{21}{Test set accuracy (in \%), memory usage and training time on the UEA EigenWorms dataset for depths 1-3 and a small selection of step sizes. The bold values denote that the model was the top performer for that step size.\relax }{table.caption.35}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces The RMSE scores on the test set for each of the vitals signs prediction tasks (RR, HR, SpO$_2$) on the BIDMC dataset. The memory usage is given as the mean over all three of the tasks as it was approximately the same for any task for a given depth and step. The bold values denote the algorithm with the lowest test set loss for a fixed step size for each task.\relax }}{22}{table.caption.36}}
\newlabel{tab:bidmc_all}{{6}{22}{The RMSE scores on the test set for each of the vitals signs prediction tasks (RR, HR, SpO$_2$) on the BIDMC dataset. The memory usage is given as the mean over all three of the tasks as it was approximately the same for any task for a given depth and step. The bold values denote the algorithm with the lowest test set loss for a fixed step size for each task.\relax }{table.caption.36}{}}
